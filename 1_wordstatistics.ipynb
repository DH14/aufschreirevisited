{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Analysing #aufschrei with Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First of all, load all packages required for this analysis. (This list will be extended in the future - make sure you run it at each start.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pymongo\n",
      "import json\n",
      "import os\n",
      "import nltk\n",
      "import re\n",
      "from nltk.collocations import *\n",
      "from string import punctuation\n",
      "from nltk.draw import dispersion_plot"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If necessary: Download NLTK data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#nltk.download('stopwords')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we will connect to the database. Please make sure you have either a local MongoDB instance running on your computer that contains the tweets, or an open SSH port forwarding connection to an existing database. (See README for details.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "con = pymongo.Connection('127.0.0.1', port=27017)\n",
      "aufschrei = con.aufschreirevisited.tweets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "A quick overview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's print the total number of tweets in the database."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count = aufschrei.count()\n",
      "print(count,\"Tweets total.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's find out how many of these tweets have been retweeted."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count_rt = aufschrei.find({\"retweeted_status\": {\"$exists\":\"true\"}}).count()\n",
      "per = 100*count_rt/count\n",
      "print(count_rt,\"Tweets were retweeted. (\"+str(per)[:4]+\"%)\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Analysing all tweets as a text corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Loading all the tweets' text from the database into a corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cursor = aufschrei.find({},{\"text\":1})\n",
      "tweets = []    # a collection\n",
      "alltweets = '' # a text object\n",
      "for tweet in cursor:\n",
      "    tweets.append(tweet['text'])\n",
      "    alltweets += tweet['text'] + ' '"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Tokenizing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First of all, we have to write our own Twitter compatible tokenizer function that will preserve hashtags and usernames instead of treating them as regular words (as most tokenizers do)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def twitter_tokenize(text):\n",
      "    text = re.sub('&.+?;',' ',text) # remove html entities\n",
      "    text = re.sub('https?:.+? ',' ',text) # remove URLs\n",
      "    tokens = re.split('[\\s.,!?:()\u201c\"]',text)\n",
      "    tok2 = []\n",
      "    pattern = re.compile('^[^\\w_@#]',re.UNICODE) # matches all special chars except #@_\n",
      "    replacepattern = re.compile('^[^\\w_@#]*',re.UNICODE) # multiple occurrences of special chars except #@_\n",
      "    for t in tokens:\n",
      "        if len(re.findall('\\w.*',t)) > 0: # clear tokens with only interpunctuation\n",
      "            if re.match(pattern,t): # find at the beginning of the word\n",
      "                t = re.sub(replacepattern,'',t)\n",
      "            t = t[::-1]\n",
      "            if re.match(pattern,t): # find at the end of the word\n",
      "                t = re.sub(replacepattern,'',t)\n",
      "            t = t[::-1]\n",
      "            tok2.append(t)\n",
      "    return tok2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* First, make the whole corpus lowercase (this step is needed for unification; because some users write all their tweets lowercase and some do not)\n",
      "* Next, tokenizing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alltweets = alltweets.lower() # all lowercase\n",
      "tokens = twitter_tokenize(alltweets) # tokenizing (you can use different tokenizing functions, e.g. nltk.word_tokenize())\n",
      "print(tokens[:100])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which tokens still contain special characters somewhere?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pat2 = re.compile('[^\\w_@#]',re.UNICODE)\n",
      "for t in tokens:\n",
      "    if re.findall(pat2,t):\n",
      "        print(t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looks good!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Counting word frequencies"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Counting the tokens with to create a frequency distribution\n",
      "* Finally, sorting the frequency distribution and printing it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = nltk.Text(tokens)\n",
      "raw_freqs = nltk.FreqDist(text) # frequency distribution"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Print counts for certain words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw_freqs['aufschrei']\n",
      "raw_freqs['#aufschrei']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rf_sort = sorted(raw_freqs, key=raw_freqs.get, reverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# printing the frequency list\n",
      "for rank, item in enumerate(rf_sort):\n",
      "    print(rank, item, raw_freqs[item])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Removing stopwords"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A new function that removes tokens containing stopwords:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_sw(corpus):\n",
      "    from nltk.corpus import stopwords\n",
      "    mysw = stopwords.words('german') + ['dass','rt','http']\n",
      "    # remove if stopword\n",
      "    corpus = [t for t in corpus if t.lower() not in mysw]\n",
      "    return corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Counting and printing again"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens_clean = remove_sw(tokens) # apply stopword filter\n",
      "text = nltk.Text(tokens_clean)\n",
      "freqs = nltk.FreqDist(text) # frequency distribution\n",
      "# sorting by frequency\n",
      "f_sort = sorted(freqs, key=freqs.get, reverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#printing the frequency list\n",
      "for rank, item in enumerate(f_sort):\n",
      "    print(rank, item, freqs[item])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Number of words in the text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Number of unique words in the text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(raw_freqs.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Number of unique words in the text, ignoring stopwords"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(freqs.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Extracting Usernames (=Mentions) and Hashtags"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hashtags = []\n",
      "mentions = []\n",
      "words = []\n",
      "for item in freqs:\n",
      "    if item.startswith('#'):\n",
      "        hashtags.append(item)\n",
      "    elif item.startswith('@'):\n",
      "        mentions.append(item)\n",
      "    else:\n",
      "        words.append(item)\n",
      "\n",
      "print('words:',len(words),'hashtags:',len(hashtags),'mentions:',len(mentions))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Hashtags:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fh_sort = sorted(hashtags, key=freqs.get, reverse=True)\n",
      "for rank, item in enumerate(fh_sort):\n",
      "    print(rank, item, freqs[item])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Usernames:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fm_sort = sorted(mentions, key=freqs.get, reverse=True)\n",
      "for rank, item in enumerate(fm_sort):\n",
      "    print(rank, item, freqs[item])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Regular words:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fw_sort = sorted(words, key=freqs.get, reverse=True)\n",
      "for rank, item in enumerate(fw_sort):\n",
      "    print(rank, item, freqs[item])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Text concordance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Print examples for the use of certain words, e.g. \"frauen\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = nltk.Text(tokens)\n",
      "text.concordance('frauen')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text.concordance('m\u00e4nner')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text.concordance('139') # example for hashtag spam"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text.concordance('homos')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Common contexts of words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text.common_contexts(words=['frauen'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text.common_contexts(words=['m\u00e4nner'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Cooccurrences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text.collocations()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Distribution of words over time"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# persons\n",
      "text.dispersion_plot(words=['br\u00fcderle','marthadear','wizorek','alice','schwarzer'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# debate/topic\n",
      "text.dispersion_plot(words=['internet','twitter','debatte','bel\u00e4stigung'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# on-topic words\n",
      "text.dispersion_plot(words=['diskriminiert','diskriminieren','\u00fcbergriffe','angst','vergewaltigung'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# potential hate speech\n",
      "text.dispersion_plot(words=['k\u00fcche','emanze','emanzen','feminazi','feminazis','fotze','fotzen',\n",
      "                            'hysterie','hysterisch','hysterische','jammern','gejammer',\n",
      "                            'verschw\u00f6rung','terror',\n",
      "                            'homos','gutmenschen','gegenschrei'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# places\n",
      "text.dispersion_plot(words=['bushaltestelle','haltestelle','bahn','heimweg','disco','konzert','caf\u00e9'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# places/institutions\n",
      "text.dispersion_plot(words=['schule','arbeit','uni','universit\u00e4t','sport','ausbildung','azubi',\n",
      "                            'betrieb'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# potential harassers\n",
      "text.dispersion_plot(words=['chef','kollege','kollegen','lehrer','arzt','trainer'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# common words in context\n",
      "text.dispersion_plot(words=['mutter','vater','bruder','schwester','kind','kinder'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# \n",
      "text.dispersion_plot(words=['arschl\u00f6cher','jungs','m\u00e4dels','m\u00e4dchen'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}