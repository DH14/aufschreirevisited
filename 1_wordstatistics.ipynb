{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Analysing #aufschrei with Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First of all, load all packages required for this analysis. (This list will be extended in the future - make sure you run it at each start.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pymongo\n",
      "import json\n",
      "import os\n",
      "import nltk\n",
      "import re\n",
      "from nltk.collocations import *\n",
      "from string import punctuation\n",
      "from nltk.draw import dispersion_plot"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If necessary: Uncomment this line to download NLTK data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#nltk.download('stopwords')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we will connect to the database. Please make sure you have either a local MongoDB instance running on your computer that contains the tweets, or an open ssh port forwarding connection to an existing database. (See README for details.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "con = pymongo.Connection('127.0.0.1', port=27017)\n",
      "aufschrei = con.aufschreirevisited.tweets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A quick overview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's print the total number of tweets in the database."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count = aufschrei.count()\n",
      "print(count,\"Tweets total.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's find out how many of these tweets have been retweeted.\n",
      "\n",
      "Nb: The Twitter API does not recognize manual (copy+paste) Retweets. We use a regular expression matching for the sequence \"RT\" at the beginning of a Tweet to bypass this limitation and catch these Retweets as well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rtre = re.compile(\"^RT\",re.IGNORECASE) # matches manual RTs\n",
      "count_rt = aufschrei.find({\"$or\": [{\"retweeted_status\": {\"$exists\":True}}, {\"text\" : rtre}]}).count()\n",
      "per = 100*count_rt/count\n",
      "print(count_rt,\"Tweets are Retweets. (\"+str(per)[:4]+\"%)\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Analysing all tweets as a text corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Loading all the tweets' text from the database into a NLTK text corpus object."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "aufschrei.create_index('created_at', pymongo.ASCENDING)\n",
      "cursor = aufschrei.find({},{'text' : 1}).sort('created_at', 1) # sort by date of publication\n",
      "tweets = []    # a collection\n",
      "alltweets = '' # a text object\n",
      "for tweet in cursor:\n",
      "    tweets.append(tweet['text'])\n",
      "    alltweets += tweet['text'] + ' \\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Same for unique Tweets (unique = not considering Retweets)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nortre = re.compile(\"^(?!RT).+\",re.IGNORECASE) # matches everything thats not a manual RT\n",
      "cursor = aufschrei.find({\"retweeted_status\": {\"$exists\":False}, \"text\":nortre},{'text' : 1}).sort('created_at', 1)\n",
      "utweets = []    # a collection\n",
      "uniquetweets = '' # a text object\n",
      "for tweet in cursor:\n",
      "    utweets.append(tweet['text'])\n",
      "    uniquetweets += tweet['text'] + ' \\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check what we've got"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(tweets[:10])\n",
      "print()\n",
      "print(utweets[:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Close the database connection."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "con.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Tokenizing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First of all, we have to write our own Twitter compatible tokenizer function that will preserve #hashtags and @usernames instead of treating them as regular words (as most tokenizers would do)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def twitter_tokenize(text):\n",
      "    text = re.sub('&.+?;',' ',text) # remove html entities\n",
      "    text = re.sub('https?:.+? ','_URL_ ',text) # remove URLs\n",
      "    text = re.sub('https?:.+?$','_URL_',text) # remove URLs\n",
      "    tokens = re.split('[\\s.,;!?:()\\[\\]\\|\u201c\"\u2026=+/]',text)\n",
      "    tok2 = []\n",
      "    # matches all special chars except #@_\n",
      "    pattern = re.compile('^[^\\w_@#]',re.UNICODE)\n",
      "    # multiple occurrences of special chars except #@_\n",
      "    replacepattern = re.compile('^[^\\w_@#]*',re.UNICODE)\n",
      "    for t in tokens:\n",
      "        if len(re.findall('\\w.*',t)) > 0: # clear tokens consisting of only special chars\n",
      "            if re.match(pattern,t): # find pattern at the beginning of the word\n",
      "                t = re.sub(replacepattern,'',t)\n",
      "            t = t[::-1] # reverse token\n",
      "            if re.match(pattern,t): # find at the end of the word\n",
      "                t = re.sub(replacepattern,'',t)\n",
      "            t = t[::-1] # flip back\n",
      "            tok2.append(t)\n",
      "    return tok2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* First, make the whole corpus lowercase (this step is needed for unification; because some users write all their tweets lowercase and some do not)\n",
      "* Next, tokenizing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alltweets = alltweets.lower() # all lowercase\n",
      "tokens = twitter_tokenize(alltweets) # tokenizing\n",
      "print(tokens[:100])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# same for unique tweets\n",
      "uniquetweets = uniquetweets.lower() # all lowercase\n",
      "utweets = [t.lower() for t in utweets]\n",
      "utokens = twitter_tokenize(uniquetweets) # tokenizing\n",
      "print(utokens[:100])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which tokens still contain special characters somewhere?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pat2 = re.compile('[^\\w_@#-]',re.UNICODE)\n",
      "specctokens = set([])\n",
      "for t in tokens:\n",
      "    if re.findall(pat2,t):\n",
      "        specctokens.add(t)\n",
      "specctokens = list(specctokens)\n",
      "specctokens = sorted(specctokens)\n",
      "print(specctokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looks good!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Counting word frequencies"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Counting the tokens with to create a frequency distribution\n",
      "* Finally, sorting the frequency distribution and printing it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = nltk.Text(tokens)\n",
      "raw_freqs = nltk.FreqDist(text) # frequency distribution"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "utext = nltk.Text(utokens)\n",
      "uraw_freqs = nltk.FreqDist(utext) # frequency distribution"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Print counts for certain words (choose your own)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(raw_freqs['aufschrei'])\n",
      "print(raw_freqs['#aufschrei'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "print(uraw_freqs['aufschrei'])\n",
      "print(uraw_freqs['#aufschrei'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sorting the list by frequency and printing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rf_sort = sorted(raw_freqs, key=raw_freqs.get, reverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# printing the frequency list\n",
      "for rank, item in enumerate(rf_sort):\n",
      "    print(rank, item, raw_freqs[item])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "urf_sort = sorted(uraw_freqs, key=uraw_freqs.get, reverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# printing the frequency list\n",
      "for rank, item in enumerate(urf_sort):\n",
      "    print(rank, item, uraw_freqs[item])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Removing stopwords"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A new function that removes tokens containing stopwords:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_sw(corpus):\n",
      "    from nltk.corpus import stopwords\n",
      "    mysw = stopwords.words('german') + ['dass']\n",
      "    # remove if stopword\n",
      "    corpus = [t for t in corpus if t.lower() not in mysw]\n",
      "    return corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Counting and printing again"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens_clean = remove_sw(tokens) # apply stopword filter\n",
      "ctext = nltk.Text(tokens_clean)\n",
      "freqs = nltk.FreqDist(ctext) # frequency distribution\n",
      "# sorting by frequency\n",
      "f_sort = sorted(freqs, key=freqs.get, reverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#printing the frequency list\n",
      "for rank, item in enumerate(f_sort):\n",
      "    print(rank, item, freqs[item])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Same steps for unique tweets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "utokens_clean = remove_sw(utokens) # apply stopword filter\n",
      "uctext = nltk.Text(utokens_clean)\n",
      "ufreqs = nltk.FreqDist(uctext) # frequency distribution\n",
      "# sorting by frequency\n",
      "uf_sort = sorted(ufreqs, key=ufreqs.get, reverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#printing the frequency list\n",
      "for rank, item in enumerate(uf_sort):\n",
      "    print(rank, item, ufreqs[item])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Total number of words resp tokens in the text (for all tweets / unique tweets)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('{} / {}'.format(len(tokens),len(utokens)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Number of unique words in the text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('{} / {}'.format(len(raw_freqs.keys()),len(uraw_freqs.keys())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Number of unique words in the text, stopwords excluded"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('{} / {}'.format(len(freqs.keys()),len(ufreqs.keys())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Extracting Usernames (=Mentions) and Hashtags"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hashtags = []\n",
      "mentions = []\n",
      "words = []\n",
      "for item in freqs:\n",
      "    if item.startswith('#'):\n",
      "        hashtags.append(item)\n",
      "    elif item.startswith('@'):\n",
      "        mentions.append(item)\n",
      "    else:\n",
      "        words.append(item)\n",
      "\n",
      "print('words:',len(words),'hashtags:',len(hashtags),'mentions:',len(mentions))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "uhashtags = []\n",
      "umentions = []\n",
      "uwords = []\n",
      "for item in ufreqs:\n",
      "    if item.startswith('#'):\n",
      "        uhashtags.append(item)\n",
      "    elif item.startswith('@'):\n",
      "        umentions.append(item)\n",
      "    else:\n",
      "        uwords.append(item)\n",
      "\n",
      "print('words:',len(uwords),'hashtags:',len(uhashtags),'mentions:',len(umentions))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Hashtags:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fh_sort = sorted(hashtags, key=freqs.get, reverse=True)\n",
      "for rank, item in enumerate(fh_sort):\n",
      "    print(rank, item, freqs[item])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "ufh_sort = sorted(uhashtags, key=ufreqs.get, reverse=True)\n",
      "for rank, item in enumerate(ufh_sort):\n",
      "    print(rank, item, ufreqs[item])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Usernames:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fm_sort = sorted(mentions, key=freqs.get, reverse=True)\n",
      "for rank, item in enumerate(fm_sort):\n",
      "    print(rank, item, freqs[item])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "ufm_sort = sorted(umentions, key=ufreqs.get, reverse=True)\n",
      "for rank, item in enumerate(ufm_sort):\n",
      "    print(rank, item, ufreqs[item])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Regular words:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fw_sort = sorted(words, key=freqs.get, reverse=True)\n",
      "for rank, item in enumerate(fw_sort):\n",
      "    print(rank, item, freqs[item])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "ufw_sort = sorted(uwords, key=ufreqs.get, reverse=True)\n",
      "for rank, item in enumerate(ufw_sort):\n",
      "    print(rank, item, ufreqs[item])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Text concordance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Print examples for the use of certain words, e.g. \"frauen\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text.concordance('frauen')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "utext.concordance('frauen')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unique Tweets have less duplicates."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "utext.concordance('m\u00e4nner')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "utext.concordance('139') # example for hashtag spam"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "utext.concordance('homos')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text.concordance('#gegenschrei',width=150,lines=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "utext.concordance('#gegenschrei',width=150,lines=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Common contexts of words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text.common_contexts(words=['frauen'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "utext.common_contexts(words=['frauen'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text.common_contexts(words=['m\u00e4nner'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "utext.common_contexts(words=['m\u00e4nner'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Collocations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text.collocations()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "utext.collocations()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Distribution of words over time / Dispersion"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# notable persons\n",
      "text.dispersion_plot(words=['br\u00fcderle','@marthadear','anne','wizorek','alice','schwarzer'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "# notable persons\n",
      "utext.dispersion_plot(words=['br\u00fcderle','@marthadear','anne','wizorek','alice','schwarzer'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Interesting! The plot above shows some irregularities between word 380000 and 425000. We'll have a look at that later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# debate/topic\n",
      "text.dispersion_plot(words=['internet','twitter','debatte','bel\u00e4stigung'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# on-topic words\n",
      "text.dispersion_plot(words=['diskriminiert','diskriminieren','\u00fcbergriffe','angst','vergewaltigung'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# potential hate speech\n",
      "text.dispersion_plot(words=['k\u00fcche','emanze','emanzen','feminazi','feminazis','fotze','fotzen',\n",
      "                            'hysterie','hysterisch','hysterische','jammern','gejammer',\n",
      "                            'verschw\u00f6rung','terror',\n",
      "                            'homos','gutmenschen','#gegenschrei'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# places\n",
      "text.dispersion_plot(words=['bushaltestelle','haltestelle','bahn','heimweg','disco','konzert','caf\u00e9','stra\u00dfenbahn',\n",
      "                            'bus','auto'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# places/institutions\n",
      "text.dispersion_plot(words=['schule','arbeit','uni','universit\u00e4t','sport','ausbildung','azubi',\n",
      "                            'betrieb'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# persons\n",
      "p_male = [w for w in words if re.search('lehrer',w) and not re.search('in$|innen$',w)]\n",
      "p_male += ['typ','chef','kollege','kollegen','arzt','trainer']\n",
      "print(p_male)\n",
      "text.dispersion_plot(words=p_male)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# persons\n",
      "p_female = [w for w in words if re.search('lehrer',w) and re.search('in$|innen$',w)]\n",
      "p_female += ['chefin','kollegin','kolleginnen','\u00e4rztin','trainerin']\n",
      "print(p_female)\n",
      "text.dispersion_plot(words=p_female)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another interesting point. Let's have closer a look at the common contexts and concordance of \"kollegin\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "utext.common_contexts(words=['kollegin'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "utext.concordance('kollegin',width=150,lines=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It looks like they're not doers, but people are reporting sexist things that happened to their colleagues under the hashtag #aufschrei as well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# common words in context\n",
      "text.dispersion_plot(words=['mutter','vater','bruder','schwester','kind','kinder','freund','freunde','freundin',\n",
      "                            'freundinnen'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text.common_contexts(words=['freund'])\n",
      "text.common_contexts(words=['freundin'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "utext.concordance('freund')\n",
      "utext.concordance('freundin')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "utext.dispersion_plot(words=['#aufschrei','#gegenschrei','gegenschrei','_URL_'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Examining the irregularities between word 380000 and 425000"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t2 = utext[380000:425000]\n",
      "t2 = remove_sw(t2) #remove stopwords\n",
      "t2 = nltk.Text(t2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# notable persons\n",
      "t2.dispersion_plot(words=['br\u00fcderle','@marthadear','anne','wizorek','alice','schwarzer'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Examine which words are frequent in this share of Tweets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t2rf = nltk.FreqDist(t2)\n",
      "# sorting by frequency\n",
      "t2rf_sort = sorted(t2rf, key=t2rf.get, reverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# printing the frequency list\n",
      "for rank, item in enumerate(t2rf_sort):\n",
      "    print(rank, item, t2rf[item])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t2.concordance('#sexisten')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t2.concordance('iphone')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's visualize the whole timeline again, consindering some of the words we just found. \n",
      "(Seems like we found an explanation for the gap.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#unique\n",
      "# notable persons\n",
      "utext.dispersion_plot(words=['br\u00fcderle','@marthadear','anne','wizorek','alice','schwarzer',\n",
      "                             '@tobiashuch','#fdp-mann','#sexisten','#lanz','#jauch',\n",
      "                             'iphone','insolvenzeinkauf','#gegenschrei','drecksau'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}